<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Olivier Wulveryck&#39;s Tech Blog</title>
    <link>https://blog.owulveryck.info/tags/machine-learning.html</link>
    <description>Recent content in Machine Learning on Olivier Wulveryck&#39;s Tech Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>olivier.wulveryck@gmail.com (Olivier Wulveryck)</managingEditor>
    <webMaster>olivier.wulveryck@gmail.com (Olivier Wulveryck)</webMaster>
    <copyright>All rights reserved - 2015/2016</copyright>
    <lastBuildDate>Fri, 20 May 2016 12:50:59 +0200</lastBuildDate>
    <atom:link href="https://blog.owulveryck.info/tags/machine-learning.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Which solution should I choose? Don&#39;t think too much and ask a bot!</title>
      <link>https://blog.owulveryck.info/2016/05/20/which-solution-should-i-choose-dont-think-too-much-and-ask-a-bot/index.html</link>
      <pubDate>Fri, 20 May 2016 12:50:59 +0200</pubDate>
      <author>olivier.wulveryck@gmail.com (Olivier Wulveryck)</author>
      <guid>https://blog.owulveryck.info/2016/05/20/which-solution-should-i-choose-dont-think-too-much-and-ask-a-bot/index.html</guid>
      <description>

&lt;h1 id=&#34;let-me-tell-you-a-story-the-why&#34;&gt;Let me tell you a story: the why!&lt;/h1&gt;

&lt;p&gt;A year ago, one of those Sunday morning where spring starts to warm up the souls, I went, as usual to my favorite bakery.
The family tradition is to come back with a bunch of &amp;ldquo;Pains au Chocolat&amp;rdquo; (which, are, you can trust me, simply excellent).&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;hello sir, I&amp;rsquo;d like 4 of your excellent &amp;ldquo;pains au chocolat&amp;rdquo; please&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;I&amp;rsquo;m sorry, I don&amp;rsquo;t have any &amp;ldquo;pains au chocolat&amp;rdquo; nor any &amp;ldquo;croissant&amp;rdquo; anymore&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;what? How is it possible ?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;everything has been sold.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;too bad&amp;hellip;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I think to myself: &lt;em&gt;why didn&amp;rsquo;t you made more?&lt;/em&gt;. He may have read my thought and told me&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;I wish I could have foreseen&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When I left his shop, his words were echoing&amp;hellip; I wish I could have foreseen&amp;hellip; We have self-driving cars, we have the Internet,
we are a civilization that is technology advanced.
Facebook recognize your face among billions as soon as you post a photo&amp;hellip; It must be possible to foresee&amp;hellip;&lt;/p&gt;

&lt;p&gt;This is how I started to gain interest in machine learning&lt;/p&gt;

&lt;p&gt;At first I started to read some papers, then I learn (a very little bit) about graph theory, Bayesian networks, Markov chains.
But I was not accurate and I felt I was missing some basic theory.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s the main reason why, 8 weeks ago, I signed in a course about &lt;a href=&#34;https://www.coursera.org/learn/machine-learning&#34;&gt;&amp;ldquo;Machine learning&amp;rdquo; on Coursera&lt;/a&gt;.
This course is given by &lt;a href=&#34;http://www.andrewng.org/&#34;&gt;Andrew Ng&lt;/a&gt; from &lt;a href=&#34;https://www.stanford.edu/&#34;&gt;Stanford University&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It is an excellent introduction that gives me all the tools I need to go deeper in this science. The course is based on real examples
and uses powerful mathematics without going too deeply in the proofs.&lt;/p&gt;

&lt;h1 id=&#34;so-what&#34;&gt;So what?&lt;/h1&gt;

&lt;p&gt;The course is not finished yet, but after about 8 weeks, I&amp;rsquo;ve learned a lot about what we call &amp;ldquo;machine learning&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;The main idea of the machine learning is:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;to feed some code with a bunch of data (who said big data was useless)&lt;/li&gt;
&lt;li&gt;to code or encode some mathematical formulas that could represent the data&lt;/li&gt;
&lt;li&gt;to implement any algorithm that optimize the formulas by minimizing the error made by the machine on the evolving data sets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To make it simple: machine learning is feeding a &amp;ldquo;robot&amp;rdquo; with data and teach him how to analyse the errors so it can make decisions on its own.&lt;/p&gt;

&lt;p&gt;Scary isn&amp;rsquo;t it? But so exciting&amp;hellip; As usual I won&amp;rsquo;t go into ethical debate on this blog, and I will stick to science and on the benefit
of the science.&lt;/p&gt;

&lt;p&gt;But indeed, always remind Fran√ßois Rabelais:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Science sans conscience n&amp;rsquo;est que ruine de l&amp;rsquo;&amp;acirc;me (&lt;em&gt;Science without conscience is but the ruin of the soul&lt;/em&gt;)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;a-use-case&#34;&gt;A use case&lt;/h2&gt;

&lt;h3 id=&#34;defining-the-problem&#34;&gt;Defining the problem&lt;/h3&gt;

&lt;p&gt;I have 4 technical solutions providing a similar goal: deliver cloud services.
Actually, none of them is fulfilling all the requirements of my business.
As usual, one is good in a certain area, while another one is weak, etc.&lt;/p&gt;

&lt;p&gt;A team of people has evaluated more than 100 criteria, and gave two quotations per criteria and per product:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the first quotation is in the range 0/3 and indicated whether the product is fulfilling the current feature&lt;/li&gt;
&lt;li&gt;the second quotation may be {0,1,3,9} and points the effort needed to reach a 3 for the feature&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore, for each solution, I have a table looking like this :&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;feature  name&lt;/th&gt;
&lt;th&gt;feature evaluation&lt;/th&gt;
&lt;th&gt;effort&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;feature 1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;feature 2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;feature 3&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;feature 4&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&amp;hellip;.&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;feature 100&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I&amp;rsquo;ve been asked to evaluate the product and to produce a comparison.&lt;/p&gt;

&lt;p&gt;To do an analytic, I must look for an element of comparison. So I&amp;rsquo;ve turned the problem into this :&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;I would like to know which product is the cheapest to fulfill my requirement.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(I&amp;rsquo;ve uploaded my samples here):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.owulveryck.info/assets/ml/solution1.csv&#34;&gt;solution 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.owulveryck.info/assets/ml/solution2.csv&#34;&gt;solution 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.owulveryck.info/assets/ml/solution3.csv&#34;&gt;solution 3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.owulveryck.info/assets/ml/solution4.csv&#34;&gt;solution 4&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;finding-a-solution&#34;&gt;Finding a solution&lt;/h3&gt;

&lt;p&gt;In the machine learning, we notice two different fields of application:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;regression&lt;/li&gt;
&lt;li&gt;classification&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The classification mechanism would be used to answer a yes/no question; for example: &lt;em&gt;should I keep solution 1&lt;/em&gt; ?
The regression mechanism helps us for &amp;ldquo;predicting&amp;rdquo;. Actually, the goal is to &lt;em&gt;automatically&lt;/em&gt; find a mathematical formulae that turns
a set of feature into a result.&lt;/p&gt;

&lt;p&gt;what is a feature, and what&amp;rsquo;s the result?
Let&amp;rsquo;s go back to my &lt;em&gt;petits pains&lt;/em&gt; example.&lt;/p&gt;

&lt;p&gt;Consider that the baker has made statistics on its production for sunday, and it has taken some events into consideration:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sunday the 1st: it was raining, I sold only 100 petits Pains&lt;/li&gt;
&lt;li&gt;sunday the 8th: it was sunny, I sold 250 petits Pains&lt;/li&gt;
&lt;li&gt;sunday the 16th: it was sunny, and it was a special day (eg: mother&amp;rsquo;s day): 300 petits Pains&lt;/li&gt;
&lt;li&gt;sunday the 24th: it was cloudy: 150 petits Pains&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here, the baker thinks that its production must be a function of the weather and the calendar; therefore those are the two features.
What ML propose is to tell the baker how many &amp;ldquo;petits pains&amp;rdquo; he should make &lt;strong&gt;knowing&lt;/strong&gt; that it is a special day (father&amp;rsquo;s day) and that it
is partially sunny&amp;hellip;&lt;/p&gt;

&lt;p&gt;Back in the context of this post, the goal of the regression would be to find a mathematical function that will tell me the effort needed
for any value, and doing this on the simple basis of the training set I have.&lt;/p&gt;

&lt;h4 id=&#34;the-actual-score-of-all-the-solutions&#34;&gt;The actual score of all the solutions&lt;/h4&gt;

&lt;p&gt;The first thing to find it the total score of all the 4 solutions.
If I consider $m$ features, the total score of the solution is defined by:&lt;/p&gt;

&lt;p&gt;$ score = \frac{1}{m} . \sum_{k=1}^{m} feature_k $&lt;/p&gt;

&lt;p&gt;What I need now, is to evaluate the effort needed to reach a score of 3 for each solution.
Let&amp;rsquo;s do that.&lt;/p&gt;

&lt;h4 id=&#34;representing-the-training-set&#34;&gt;Representing the training set&lt;/h4&gt;

&lt;p&gt;First, let&amp;rsquo;s plot the training set.
&lt;center&gt;
&lt;img class=&#34;img-responsive&#34; src=&#34;https://blog.owulveryck.info/assets/images/ml/trainingset.jpg&#34; alt=&#34;Training set&#34;/&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;note&lt;/strong&gt; the representation is not accurate because there may be several bunk points&lt;/p&gt;

&lt;p&gt;I will use in this post what&amp;rsquo;s called &amp;ldquo;supervised learning&amp;rdquo;. That means that I will express a skeleton of function and let the machine
adjust it. (actually this is a very basic and week implementation; a lot more complex examples may be implemented but that&amp;rsquo;s not the purpose of this post)&lt;/p&gt;

&lt;p&gt;When I look at the training set representation, I can imagine a line passing by the middle of the plots.
This line may look like this:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;img class=&#34;img-responsive&#34; src=&#34;https://blog.owulveryck.info/assets/images/ml/x-1_5.jpg&#34; alt=&#34;x^(-1/5)&#34;/&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;This is actually a representation of the function $ x^{\frac{1}{5}} $&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s assume that this function may basically fit my example, my goal will be to adapt the function.
assume this equation with two parameters $\theta_0$ and $\theta_1$ that will influence the curve:&lt;/p&gt;

&lt;p&gt;$ f(x) = \theta_0 + \theta_1 . x^{\frac{1}{5}} $&lt;/p&gt;

&lt;p&gt;Therefore, my goal will be to code something so that the machine will figure out what $\theta_0$ and $\theta_1$  are.&lt;/p&gt;

&lt;p&gt;I will use an implementation of an algorithm called &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_descent&#34;&gt;gradient descent&lt;/a&gt; for linear regression.
I won&amp;rsquo;t go into the details of this algorithm, as it takes a complete course to be explained.&lt;/p&gt;

&lt;p&gt;The implementation is made with &lt;a href=&#34;https://www.gnu.org/software/octave/&#34;&gt;GNU octave&lt;/a&gt; and the code is available on my &lt;a href=&#34;https://github.com/owulveryck/linear-regression-example&#34;&gt;github&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-computation-and-the-result&#34;&gt;The computation and the result&lt;/h2&gt;

&lt;p&gt;Here is a figure representing the function for one particular solution:
&lt;center&gt;
&lt;img class=&#34;img-responsive&#34; src=&#34;https://blog.owulveryck.info/assets/images/ml/trainingset_plot.jpg&#34; alt=&#34;Training set with the function&#34;/&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;We can see that the curve is &amp;ldquo;under fitting&amp;rdquo; the data.
Anyway, let&amp;rsquo;s continue and get the result I want (I will explain later how to perform better):&lt;/p&gt;

&lt;p&gt;Here are the computational results:
&lt;pre&gt;
octave:10&amp;gt; compute
Analysing solution1.csv:0.&lt;sup&gt;67&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;
Running gradient descent&amp;hellip;
Theta found by gradient descent: 5.397050 -4.315835
Prediction for x=0.669291 ; 1.414256
Prediction for x=3 ; 0.020681
Effort (scaled to 10): 2.582105&lt;/p&gt;

&lt;p&gt;Analysing solution2.csv:0.&lt;sup&gt;96&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;
Running gradient descent&amp;hellip;
Theta found by gradient descent: 3.178478 -2.451611
Prediction for x=0.960630 ; 0.746482
Prediction for x=3 ; 0.124430
Effort (scaled to 10): 1.957075&lt;/p&gt;

&lt;p&gt;Analysing solution3.csv:0.&lt;sup&gt;67&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;
Running gradient descent&amp;hellip;
Theta found by gradient descent: 2.557847 -2.015334
Prediction for x=0.669291 ; 0.698031
Prediction for x=3 ; 0.047283
Effort (scaled to 10): 2.544122&lt;/p&gt;

&lt;p&gt;Analysing solution4.csv:0.&lt;sup&gt;86&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;
Running gradient descent&amp;hellip;
Theta found by gradient descent: 3.104868 -2.422627
Prediction for x=0.858268 ; 0.755175
Prediction for x=3 ; 0.086926
Effort (scaled to 10): 2.152261
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;For each solution, I have:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the score (the first line /3)&lt;/li&gt;
&lt;li&gt;the parameters $\theta$&lt;/li&gt;
&lt;li&gt;a prediction for the actual score, and for a score of 3&lt;/li&gt;
&lt;li&gt;the effort (scale on 10) needed to pass from the actual score to 3&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;final-result&#34;&gt;Final result&lt;/h4&gt;

&lt;p&gt;Here is the final classification of my four solutions:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Solution&lt;/th&gt;
&lt;th&gt;score&lt;/th&gt;
&lt;th&gt;effort&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Solution 2&lt;/td&gt;
&lt;td&gt;0.96&lt;/td&gt;
&lt;td&gt;1.95&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Solution 4&lt;/td&gt;
&lt;td&gt;0.86&lt;/td&gt;
&lt;td&gt;2.15&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Solution 3&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;td&gt;2.54&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Solution 1&lt;/td&gt;
&lt;td&gt;0.67&lt;/td&gt;
&lt;td&gt;2.58&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Solution 2 is the cheapest. It&amp;rsquo;s possible to go into further analysis to determine why it&amp;rsquo;s the cheapest, and how the other ones
may catch up and go back in the race, but again, that is not the purpose of my post.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;This is a simple approach.
Some axis of optimization could be to use a more complex polynomial (eg: $\theta_0+\theta_1.x^\frac{1}{3}+\theta_2.x^\frac{1}{5}$)
or to use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Support_vector_machine&#34;&gt;support vector machine&lt;/a&gt; with a gaussian kernel for example.&lt;/p&gt;

&lt;p&gt;One other optimization would be to add some more features, such as, for example, a score on the importance of a feature (a functional feature).&lt;/p&gt;

&lt;p&gt;Machine learning is a wide mathematical and IT area. It is now in everyone&amp;rsquo;s life.
Nowadays we are talking about plateform fully automated, self-healing applications, smart deployements, smart monitoring.
There are already some good implementations of algorithms on the market, but there is a huge place for integration of those tools into
the life of the IT specialist.&lt;/p&gt;

&lt;p&gt;Automation has already helped and took the boring job of the IT specialist. Smart automation will go a step further.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Is there a Markov model hidden in the choreography?</title>
      <link>https://blog.owulveryck.info/2016/02/29/is-there-a-markov-model-hidden-in-the-choreography/index.html</link>
      <pubDate>Mon, 29 Feb 2016 20:55:01 +0100</pubDate>
      <author>olivier.wulveryck@gmail.com (Olivier Wulveryck)</author>
      <guid>https://blog.owulveryck.info/2016/02/29/is-there-a-markov-model-hidden-in-the-choreography/index.html</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;In my last post I introduced the notion of choreography as a way to deploy an manage application.
It could be possible to implement self-healing, elasticity and in a certain extent
self awareness.&lt;/p&gt;

&lt;p&gt;To do so, we must not rely on the &lt;em&gt;certainty&lt;/em&gt; and the &lt;em&gt;determinism&lt;/em&gt; of the automated tasks.
&lt;em&gt;Mark Burgess&lt;/em&gt; explains in his book &lt;a href=&#34;http://http://www.amazon.com/gp/product/1491923075/ref=pd_lpo_sbs_dp_ss_1?pf_rd_p=1944687522&amp;amp;pf_rd_s=lpo-top-stripe-1&amp;amp;pf_rd_t=201&amp;amp;pf_rd_i=1492389161&amp;amp;pf_rd_m=ATVPDKIKX0DER&amp;amp;pf_rd_r=1BRFTEAZ2RRQ8M77MZ0C&#34;&gt;in search of certainty&lt;/a&gt; that none should consider the command and control anymore.&lt;/p&gt;

&lt;p&gt;Actually we grew up with the idea that a computer will do whatever we told him to.
The truth is that it simply don&amp;rsquo;t. If that sounds astonishing to you, just consider the famous bug.
A bug is a little insect that will avoid any programmed behaviour to act as it should.&lt;/p&gt;

&lt;p&gt;In a lot of wide spread software, we find &lt;em&gt;if-then-else&lt;/em&gt; or &lt;em&gt;try-catch&lt;/em&gt; statements.
Of course one could argue that the purpose of this conditional executionis is to deal with different scenarii, which is true, but indeed,
the keyword is &lt;em&gt;try&lt;/em&gt;&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;back-to-the-choreography&#34;&gt;Back to the choreography&lt;/h2&gt;

&lt;p&gt;In the choreography principle, the automation is performed by a set of dancer that acts on their own. Actually, the most logical way
to program it, is to let them know about the execution plan, and assume that everything will run as expected.&lt;/p&gt;

&lt;p&gt;What I would like to study is simply that deployement without knowing the deployement plan.
The nodes would try to perform the task, and depending on the event they receive, they learn and enhance their probability of success.&lt;/p&gt;

&lt;h3 id=&#34;first-problem&#34;&gt;First problem&lt;/h3&gt;

&lt;p&gt;First, I&amp;rsquo;m considering a single node $A$  which can be in three states $\alpha$, $\beta$ and $\gamma$.
Let&amp;rsquo;s $S$ be the set of states such as $S = \left\{\alpha, \beta, \gamma\right\}$&lt;/p&gt;

&lt;h4 id=&#34;actually-knowing-what-s-expected&#34;&gt;Actually knowing what&amp;rsquo;s expected&lt;/h4&gt;

&lt;p&gt;The expected execution is: $ \alpha \mapsto \beta \mapsto \gamma$&lt;/p&gt;

&lt;p&gt;therefore, the transition matrix should be:&lt;/p&gt;

&lt;p&gt;$$
P=\begin{pmatrix}
0 &amp;amp; 1 &amp;amp; 0 \&lt;br /&gt;
0 &amp;amp; 0 &amp;amp; 1 \&lt;br /&gt;
0 &amp;amp; 0 &amp;amp; 0
\end{pmatrix}
$$&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s represent it with GNU-R (see &lt;a href=&#34;http://www.r-bloggers.com/getting-started-with-markov-chains/&#34;&gt;this blog post&lt;/a&gt;
for an introduction of markov reprentation with this software)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; library(expm)
&amp;gt; library(markovchain)
&amp;gt; library(diagram)
&amp;gt; library(pracma)
&amp;gt; stateNames &amp;lt;- c(&amp;quot;Alpha&amp;quot;,&amp;quot;Beta&amp;quot;,&amp;quot;Gamma&amp;quot;)
&amp;gt; ExecutionPlan &amp;lt;- matrix(c(0,1,0,0,0,1,0,0,0),nrow=3, byrow=TRUE)
&amp;gt; row.names(ExecutionPlan) &amp;lt;- stateNames; colnames(ExecutionPlan) &amp;lt;- stateNames
&amp;gt; ExecutionPlan
      Alpha Beta Gamma
      Alpha     0    1     0
      Beta      0    0     1
      Gamma     0    0     0
&amp;gt; svg(&amp;quot;ExecutionPlan.svg&amp;quot;)
&amp;gt; plotmat(ExecutionPlan,pos = c(1,2), 
         lwd = 1, box.lwd = 2, 
         cex.txt = 0.8, 
         box.size = 0.1, 
         box.type = &amp;quot;circle&amp;quot;, 
         box.prop = 0.5,
         box.col = &amp;quot;light yellow&amp;quot;,
         arr.length=.1,
         arr.width=.1,
         self.cex = .4,
         self.shifty = -.01,
         self.shiftx = .13,
         main = &amp;quot;&amp;quot;)
&amp;gt; dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which is represented by:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.owulveryck.info/assets/images/ExecutionPlan.svg&#34; alt=&#34;Representation&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;knowing-part-of-the-plan&#34;&gt;Knowing part of the plan&amp;hellip;&lt;/h4&gt;

&lt;p&gt;Now let&amp;rsquo;s consider a different scenario. I assume now that the only known hypothesis is that we cannot go
from $\alpha$ to $\gamma$ and vice-versa, but for the rest, the execution may refer to this transition matrix:&lt;/p&gt;

&lt;p&gt;$
P=\begin{pmatrix}
\frac{1}{2} &amp;amp; \frac{1}{2} &amp;amp; 0 \&lt;br /&gt;
\frac{1}{3} &amp;amp; \frac{1}{3} &amp;amp; \frac{1}{3}  \&lt;br /&gt;
0 &amp;amp; \frac{1}{2} &amp;amp; \frac{1}{2}
\end{pmatrix}
$
which is represented this way &lt;img src=&#34;https://blog.owulveryck.info/assets/images/ExecutionPlan2.svg&#34; alt=&#34;Representation&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The transition matrix is regular - we can see, for example that $P^2$ contains all non nil numbers:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; ExecutionPlan %^% 2
                Alpha     Beta      Gamma
          Alpha 0.4166667 0.4166667 0.1666667
          Beta  0.2777778 0.4444444 0.2777778
          Gamma 0.1666667 0.4166667 0.4166667
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Therefore, Makov theorem says that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;as n approaches infinity, $P^n = S$ where $S$ is a matrix of the form $[\mathbf{v}, \mathbf{v},&amp;hellip;,\mathbf{v}]$, where $\mathbf{v}$ being a constant vector&lt;/li&gt;
&lt;li&gt;let $X$ be any state vector, then we have $\lim_{n\to \infty}P^nX = \mathbf{v}$ where $\mathbf{v}$ is a fixed probability vector (the sum of its entries = 1), all whose entries are positives&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So we can look for vector $\mathbf{v}$ (also known as the &lt;strong&gt;steady-state vector of the system&lt;/strong&gt;) to see if there is a good chance that our &lt;em&gt;finite state machine&lt;/em&gt; would converged to the desired state $\gamma$.&lt;/p&gt;

&lt;h3 id=&#34;evaluation-of-the-steady-state-vector&#34;&gt;Evaluation of the steady-state vector&lt;/h3&gt;

&lt;p&gt;Now since $P^{n+1}=P*P^n$ and that both $P^{n+1}$ and $P^n$  approach $S$, we have $S=P*S$.&lt;/p&gt;

&lt;p&gt;Note that any column of this matrix equation gives $P\mathbf{v}=\mathbf{v}$. Therefore, the steady-state vector of a regular Markov chain with transition matrix $P$ is the unique probability vector $\mathbf{v}$ satisfying $P\mathbf{v}=\mathbf{v}$.&lt;/p&gt;

&lt;p&gt;To find the steady state vector, we must solve the equation: $P\mathbf{v}=\mathbf{v}$. $\mathbf{v}$ is actually an eigenvector for an eigenvalue $\lambda = 1$.&lt;/p&gt;

&lt;p&gt;_Note from &lt;a href=&#34;https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors&#34;&gt;wikipedia&lt;/a&gt;_&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In linear algebra, an eigenvector or characteristic vector of a square matrix is a vector that does not change its direction under the associated linear transformation.
In other words: if $v$ is a vector that is not zero, then it is an eigenvector of a square matrix $A$ if $Av$ is a scalar multiple of $v$. i
This condition could be written as the equation: $ Av = \lambda v$, where $\lambda$ is a scalar known as the eigenvalue or characteristic
value associated with the eigenvector $v$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To compute the eigenvector, we should find the solution to the equation $det(A-\lambda I)=0$ where $I$ is the identity matrix. Actually
I don&amp;rsquo;t know how to do it anymore, and I will simply use &lt;em&gt;R&lt;/em&gt;&amp;rsquo;s &lt;em&gt;eigen&lt;/em&gt; function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; eigen(ExecutionPlan)
$values
[1]  1.0000000  0.5000000 -0.1666667

$vectors
          [,1]          [,2]       [,3]
          [1,] 0.5773503  7.071068e-01  0.5144958
          [2,] 0.5773503  1.107461e-16 -0.6859943
          [3,] 0.5773503 -7.071068e-01  0.5144958

&amp;gt; ExecutionPlan %^% 15
        Alpha      Beta     Gamma
Alpha 0.2857295 0.4285714 0.2856990
Beta  0.2857143 0.4285714 0.2857143
Gamma 0.2856990 0.4285714 0.2857295
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait, it has found 3 eigenvalues, and one of those equals 1 which is coherent.
But the eigen vector is not coherent at all with the evaluation of the matrix at step 15.&lt;/p&gt;

&lt;p&gt;According to &lt;a href=&#34;http://stackoverflow.com/questions/14912279/how-to-obtain-right-eigenvectors-of-matrix-in-r&#34;&gt;stackoverflow&lt;/a&gt;
that&amp;rsquo;s because it computes the &lt;em&gt;right&lt;/em&gt; eigenvector and what I need is the &lt;em&gt;left&lt;/em&gt; eigenvector.&lt;/p&gt;

&lt;p&gt;Here is how to evaluate it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; lefteigen  &amp;lt;-  function(A){
       return(t(eigen(t(A))$vectors))
}
&amp;gt; lefteigen(ExecutionPlan)
               [,1]          [,2]       [,3]
          [1,] 0.4850713  7.276069e-01  0.4850713
          [2,] 0.7071068 -3.016795e-16 -0.7071068
          [3,] 0.4082483 -8.164966e-01  0.4082483
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We now have the steady vector : $\mathbf{v} = \begin{pmatrix}0.48 \\ 0.70 \\ 0.40\end{pmatrix}$&lt;/p&gt;

&lt;p&gt;which simply means that according to our theory, our finite state machin will most likely end in state $\beta$.&lt;/p&gt;

&lt;h3 id=&#34;analysis&#34;&gt;Analysis&lt;/h3&gt;

&lt;p&gt;What did I learn ?
Not that much actually. I&amp;rsquo;ve learned that given a transition matrix (a model) I could easily compute the probability of success.
If I consider the finte state machine as the whole automator of deploiement, given the pobability of failure, I can predict
if it&amp;rsquo;s worth continuing the deploiement or not.&lt;/p&gt;

&lt;p&gt;Cool, but far away from my goal: I want a distributed application to learn how to deploy, cure, and take care of itself with a single information:
its topology.&lt;/p&gt;

&lt;p&gt;Back to real life, the model I&amp;rsquo;ve described in this post could be the observable states of the application (eg: $\alpha = initial$,$\beta = configured$, $\gamma=started$&amp;hellip;)&lt;/p&gt;

&lt;p&gt;Hence, the states of the components of the application are hidden from the model (and they must remain hidden, as I don&amp;rsquo;t care observing them)&lt;/p&gt;

&lt;p&gt;And this is the proper definition of a &lt;strong&gt;hidden markov model (HMM)&lt;/strong&gt;.
So yes, there is a Markov model hidden in the choreography!&lt;/p&gt;

&lt;p&gt;I shall continue the study and learn how the signals sent from the compenent gives &lt;em&gt;evidences&lt;/em&gt; and do influence the Markov Model of my application.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s a matter of inference, I-maps, Bayesian networks, HMM&amp;hellip;. It&amp;rsquo;s about machine learning which is fascinating !&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>